{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrMNyuosropu",
        "outputId": "40e5e0d4-1d27-4254-d8ce-2d58f4f7af32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import functional as F\n",
        "import torch\n",
        "from tensorflow.keras.models import load_model\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Paths to the COCO dataset folders\n",
        "images_folder = '/content/drive/MyDrive/Colab Notebooks/OCR/Dataset/train2017'  # Replace with the path to the train2017 folder\n",
        "annotations_file = '/content/drive/MyDrive/Colab Notebooks/OCR/Dataset/annotations/captions_train2017.json'  # Replace with the path to the annotation file\n",
        "\n",
        "# Create directories for the splits if they don't exist\n",
        "train_folder = '/content/drive/MyDrive/Colab Notebooks/OCR/Dataset/train'\n",
        "val_folder = '/content/drive/MyDrive/Colab Notebooks/OCR/Dataset/val'\n",
        "test_folder = '/content/drive/MyDrive/Colab Notebooks/OCR/Dataset/test'\n",
        "\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "os.makedirs(val_folder, exist_ok=True)\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "\n",
        "# Load the COCO annotations JSON file\n",
        "with open(annotations_file, 'r') as f:\n",
        "    annotations_data = json.load(f)\n",
        "\n",
        "# Get the list of image IDs and shuffle them for randomness\n",
        "image_ids = [image['id'] for image in annotations_data['images']]\n",
        "random.shuffle(image_ids)\n",
        "\n",
        "# Split the image IDs into 3 sets (train, val, test)\n",
        "train_ids = image_ids[:int(0.7 * len(image_ids))]  # 70% for training\n",
        "val_ids = image_ids[int(0.7 * len(image_ids)):int(0.85 * len(image_ids))]  # 15% for validation\n",
        "test_ids = image_ids[int(0.85 * len(image_ids)):]  # 15% for testing\n",
        "\n",
        "# Helper function to copy images to the corresponding folders\n",
        "def copy_images(ids, source_folder, target_folder):\n",
        "    for img_id in ids:\n",
        "        # Find the image file by ID\n",
        "        img_file = next((img['file_name'] for img in annotations_data['images'] if img['id'] == img_id), None)\n",
        "        if img_file:\n",
        "            src_path = os.path.join(source_folder, img_file)\n",
        "            dst_path = os.path.join(target_folder, img_file)\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "# Copy the images for each split\n",
        "copy_images(train_ids, images_folder, train_folder)\n",
        "copy_images(val_ids, images_folder, val_folder)\n",
        "copy_images(test_ids, images_folder, test_folder)\n",
        "\n",
        "# Create new annotation files for each split (train, val, test)\n",
        "def create_split_annotation_file(ids, annotations_data, split_name):\n",
        "    new_annotations = {\n",
        "        'images': [img for img in annotations_data['images'] if img['id'] in ids],\n",
        "        'annotations': [anno for anno in annotations_data['annotations'] if anno['image_id'] in ids],\n",
        "        'categories': annotations_data['categories']\n",
        "    }\n",
        "    with open(f'{split_name}_annotations.json', 'w') as f:\n",
        "        json.dump(new_annotations, f)\n",
        "\n",
        "# Create annotation files for each split\n",
        "create_split_annotation_file(train_ids, annotations_data, 'train')\n",
        "create_split_annotation_file(val_ids, annotations_data, 'val')\n",
        "create_split_annotation_file(test_ids, annotations_data, 'test')\n",
        "\n",
        "print(f\"COCO dataset has been split into {train_folder}, {val_folder}, and {test_folder}.\")\n"
      ],
      "metadata": {
        "id": "XwMtFz71Rou-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "0f26d0fd-9675-4969-cfa2-2907bae8c829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-94b95aa21e54>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Copy the images for each split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mcopy_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mcopy_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mcopy_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-94b95aa21e54>\u001b[0m in \u001b[0;36mcopy_images\u001b[0;34m(ids, source_folder, target_folder)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0msrc_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Copy the images for each split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir_train = \"COCO-Text/images/train2014/\"\n",
        "image_dir_val = \"COCO-Text/images/val2014/\"\n",
        "output_dir = \"COCO-Text/preprocessed_images/\"\n",
        "annotations_path=\"/content/drive/MyDrive/Colab Notebooks/OCR/Dataset/annotations/captions_train2017.json\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Y8Jxqfq0yEjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load annotations\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "annotations = cocotext[\"anns\"]\n",
        "images = cocotext[\"imgs\"]\n",
        "\n",
        "# Filter legible english text\n",
        "filtered_annotations = [\n",
        "    {\n",
        "        \"image_id\": ann[\"image_id\"],\n",
        "        \"bbox\": ann[\"bbox\"],\n",
        "        \"text\": ann[\"text\"]\n",
        "    }\n",
        "    for ann in annotations[\"anns\"].values()\n",
        "    if ann[\"legibility\"] == \"legible\" and ann[\"language\"] == \"english\"\n",
        "]"
      ],
      "metadata": {
        "id": "hXe5Uj9Mr-5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_bbox(bbox, img_width, img_height):\n",
        "    x, y, w, h = bbox\n",
        "    x_min = x / img_width\n",
        "    y_min = y / img_height\n",
        "    x_max = (x + w) / img_width\n",
        "    y_max = (y + h) / img_height\n",
        "    return [x_min, y_min, x_max, y_max]"
      ],
      "metadata": {
        "id": "zAJ8t4A1x6Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean and std normalization\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "desired_size = 224\n",
        "\n",
        "for ann in filtered_annotations:\n",
        "  image_id = ann[\"image_id\"]\n",
        "  image_path = os.path.join(image_dir, image_id + \".jpg\")\n",
        "\n",
        "  width, height = image.size\n",
        "  bbox = filtered_annotations[0]['bbox']\n",
        "  normalized_bbox = normalize_bbox(bbox, width, height)\n",
        "\n",
        "  image = Image.open(image_path).convert(\"RGB\")\n",
        "  image = F.pad(image, padding=(0, 0, desired_size - image.size[0], desired_size - image.size[1]), fill=(0, 0, 0))\n",
        "  image=image/255.0\n",
        "  cropped_region = image.crop((x, y, x + w, y + h))\n",
        "  output_path = os.path.join(output_dir, f\"{image_id}.jpg\")\n",
        "  Image.fromarray((cropped_region * 255).astype(np.uint8)).save(output_path)"
      ],
      "metadata": {
        "id": "gwkPo4ZkYmWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  image_array = np.array(image) / 255.0  # Normalize to [0, 1]\n",
        "  image_array = (image_array - mean) / std"
      ],
      "metadata": {
        "id": "VDeOe9ubzzh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define augmentation pipeline\n",
        "augmentations = A.Compose([\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Rotate(limit=15, p=0.5),\n",
        "    A.Resize(224, 224),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Apply augmentations\n",
        "augmented = augmentations(image=np.array(image))\n",
        "image_augmented = augmented[\"image\"]\n"
      ],
      "metadata": {
        "id": "SVIqdWJxaP0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CRNN Model Definition\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, input_channels=1, hidden_units=256, num_classes=37):\n",
        "        super(CRNN, self).__init__()\n",
        "\n",
        "        # Convolutional Layers (CNN)\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Maxpooling\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Recurrent Layer (LSTM)\n",
        "        self.rnn = nn.LSTM(256, hidden_units, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Fully connected layers (for output)\n",
        "        self.fc = nn.Linear(hidden_units * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolutional layers\n",
        "        x = self.maxpool(torch.relu(self.conv1(x)))\n",
        "        x = self.maxpool(torch.relu(self.conv2(x)))\n",
        "        x = self.maxpool(torch.relu(self.conv3(x)))\n",
        "\n",
        "        # Prepare the data for LSTM (flatten the spatial dimensions)\n",
        "        x = x.permute(0, 3, 2, 1).contiguous()  # [batch_size, width, height, channels]\n",
        "        x = x.view(x.size(0), x.size(1), -1)  # Flatten the height and channels\n",
        "\n",
        "        # Apply LSTM (bidirectional)\n",
        "        x, _ = self.rnn(x)\n",
        "\n",
        "        # Apply fully connected layer\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example: Instantiate the model\n",
        "model = CRNN(input_channels=3, hidden_units=256, num_classes=37)  # num_classes depends on your dataset (e.g., 26 letters + 10 digits)\n"
      ],
      "metadata": {
        "id": "TJS7nr8VyyDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.001\n",
        "\n",
        "class OCRDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "\n",
        "        # Apply transformations (e.g., resizing, normalization)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get the label (text transcription)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Example: Image transformations (convert to tensor, normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 128)),  # Resize images to a fixed size (height=32, width=128)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n"
      ],
      "metadata": {
        "id": "WeR9Jeec2e5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function (CTC Loss)\n",
        "criterion = nn.CTCLoss()\n",
        "\n",
        "#optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), learning_rate=0.001)\n"
      ],
      "metadata": {
        "id": "W8zWUFgg20ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10  # Number of training epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        # Move images and labels to GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # CTC Loss requires the target to be of shape [batch_size, seq_len], and lengths of the output sequences\n",
        "        # Calculate input_lengths (for each image in batch)\n",
        "        input_lengths = torch.full((images.size(0),), outputs.size(1), dtype=torch.long)\n",
        "\n",
        "        # Calculate target_lengths (length of each text label)\n",
        "        target_lengths = torch.IntTensor([len(label) for label in labels])\n",
        "\n",
        "        # Compute CTC loss\n",
        "        loss = criterion(outputs.log_softmax(2), labels, input_lengths, target_lengths)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}\")\n"
      ],
      "metadata": {
        "id": "szF9mwui22GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/Colab Notebooks/OCR/model.keras'"
      ],
      "metadata": {
        "id": "ZLB4gODdStu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(model_path)"
      ],
      "metadata": {
        "id": "iqDYAzi6S5XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = load_model(model_path)"
      ],
      "metadata": {
        "id": "CZrNbSZDTAzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=CTCloss(), optimizer=Adam(learning_rate),metrics=['accuracy'], metrics=[CWERMetric()])"
      ],
      "metadata": {
        "id": "BjbrG2_XTCbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_ctc_output(output, characters='abcdefghijklmnopqrstuvwxyz0123456789'):\n",
        "    \"\"\" Decodes CTC output into readable text \"\"\"\n",
        "    _, max_indices = torch.max(output, dim=2)  # Get max probability indices (each index corresponds to a character)\n",
        "    decoded_text = []\n",
        "\n",
        "    prev_char = -1\n",
        "    for i in range(max_indices.size(1)):  # Loop over the sequence length\n",
        "        char_idx = max_indices[0][i].item()\n",
        "        if char_idx != prev_char:  # Avoid duplicate characters\n",
        "            decoded_text.append(characters[char_idx])\n",
        "        prev_char = char_idx\n",
        "\n",
        "    return ''.join(decoded_text)\n",
        "\n",
        "# Example inference\n",
        "image = Image.open('path/to/cropped_image.jpg').convert('RGB')\n",
        "image = transform(image).unsqueeze(0).cuda() if torch.cuda.is_available() else transform(image).unsqueeze(0)\n",
        "\n",
        "# Predict\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "    decoded_text = decode_ctc_output(output)\n",
        "    print(f\"Predicted text: {decoded_text}\")\n"
      ],
      "metadata": {
        "id": "yZSpX3wa3kFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, criterion, characters='abcdefghijklmnopqrstuvwxyz0123456789'):\n",
        "    \"\"\"Evaluates the model and computes the loss and character accuracy.\"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    correct_characters = 0\n",
        "    total_characters = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            # Move data to GPU\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calculate input lengths (assume all outputs are of the same length)\n",
        "            input_lengths = torch.full((images.size(0),), outputs.size(1), dtype=torch.long).to(device)\n",
        "\n",
        "            # Convert labels to target format and compute target lengths\n",
        "            target_texts = [label for label in labels]\n",
        "            target_lengths = torch.IntTensor([len(label) for label in target_texts]).to(device)\n",
        "\n",
        "            # Flatten the target texts into a single tensor for CTC loss\n",
        "            all_targets = ''.join(target_texts)\n",
        "            targets = torch.IntTensor([characters.index(c) for c in all_targets]).to(device)\n",
        "\n",
        "            # Compute CTC Loss\n",
        "            loss = criterion(outputs.log_softmax(2), targets, input_lengths, target_lengths)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Decode predictions\n",
        "            predicted_texts = decode_ctc_output(outputs, characters)\n",
        "\n",
        "            # Compute character-level accuracy\n",
        "            for pred_text, true_text in zip(predicted_texts, target_texts):\n",
        "                correct_characters += sum([1 for p, t in zip(pred_text, true_text) if p == t])\n",
        "                total_characters += len(true_text)\n",
        "\n",
        "            total_samples += len(target_texts)\n",
        "\n",
        "    # Compute final metrics\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    char_accuracy = correct_characters / total_characters * 100\n",
        "\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Character Accuracy: {char_accuracy:.2f}%\")\n",
        "\n",
        "    return avg_loss, char_accuracy\n",
        "\n",
        "# Define test dataset and dataloader\n",
        "test_image_paths = [\"/content/drive/MyDrive/path/to/test_image1.jpg\", \"/content/drive/MyDrive/path/to/test_image2.jpg\"]\n",
        "test_labels = [\"text1\", \"text2\"]  # Corresponding ground-truth labels for test images\n",
        "\n",
        "test_dataset = OCRDataset(test_image_paths, test_labels, transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define characters and criterion\n",
        "characters = 'abcdefghijklmnopqrstuvwxyz0123456789'\n",
        "criterion = nn.CTCLoss().to(device)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_dataloader, criterion, characters)"
      ],
      "metadata": {
        "id": "g5r89dyFUpuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the summary of model\n",
        "print(trained_model.summary())"
      ],
      "metadata": {
        "id": "p8VtLWlKTJ7E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}