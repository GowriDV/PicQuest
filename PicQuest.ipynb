{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "Kjm6lCcKguxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_model_path = '/content/drive/MyDrive/Colab Notebooks/Caption Generation/model.keras'\n",
        "ocr_model_path = '/content/drive/MyDrive/Colab Notebooks/OCR/model.keras'"
      ],
      "metadata": {
        "id": "2uPStwnkg7PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_model = load_model(caption_model_path)\n",
        "ocr_model=load_model(ocr_model_path)"
      ],
      "metadata": {
        "id": "C6BJ08xkhBj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_model.summary()"
      ],
      "metadata": {
        "id": "C0gONZYklTvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ocr_model.summary()"
      ],
      "metadata": {
        "id": "-xtp-XX7ldXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_directory='/content/drive/MyDrive/Colab Notebooks/Caption Generation'\n",
        "\n",
        "# Load features from pickle file\n",
        "pickle_file_path = os.path.join(features_directory, 'img_features.pkl')\n",
        "with open(pickle_file_path, 'rb') as file:\n",
        "    loaded_features = pickle.load(file)"
      ],
      "metadata": {
        "id": "9ijJGNfvptZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "max_length=35"
      ],
      "metadata": {
        "id": "zDpGuCMoqZdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = VGG16()\n",
        "# restructure the model\n",
        "vgg_model = Model(inputs=vgg_model.inputs,\n",
        "                  outputs=vgg_model.layers[-2].output)"
      ],
      "metadata": {
        "id": "UxKnNkqFq7Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/pngtree-real-life-hot-air-balloon-flying-in-the-sky-image_2622657.jpg'\n",
        "# load image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "# convert image pixels to numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "# preprocess image from vgg\n",
        "image = preprocess_input(image)\n",
        "# extract features\n",
        "feature = vgg_model.predict(image, verbose=0)"
      ],
      "metadata": {
        "id": "BrlYng9Oq46A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypgPaqHVgof4"
      },
      "outputs": [],
      "source": [
        "# Dataset Loading\n",
        "def load_images_from_folder(folder_path):\n",
        "    image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(('.jpg', '.png'))]\n",
        "    return image_paths\n",
        "\n",
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "         return word\n",
        "    return None\n",
        "\n",
        "# generate caption for an image\n",
        "def caption_prediction(model, image, tokenizer, max_length):\n",
        "    # load image\n",
        "    image = load_img(image_path, target_size=(224, 224))\n",
        "    # convert image pixels to numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # preprocess image from vgg\n",
        "    image = preprocess_input(image)\n",
        "    # extract features\n",
        "    image = vgg_model.predict(image, verbose=0)\n",
        "\n",
        "    # add start tag for generation process\n",
        "    in_text = 'start'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length, padding='post')\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "# Text Recognition\n",
        "def recognize_text(model, image):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text = model(image)\n",
        "    return text\n",
        "\n",
        "# Text Prompt Matching\n",
        "def find_most_relevant_image(prompt, image_paths, caption_model, ocr_model):\n",
        "\n",
        "    sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')  # Sentence embeddings for similarity\n",
        "    prompt_embedding = sentence_transformer.encode([prompt])\n",
        "\n",
        "    relevance_scores = []\n",
        "    for image_path in image_paths:\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        # Generate caption\n",
        "        caption = caption_prediction(caption_model, image, tokenizer, max_length)\n",
        "        caption_embedding = sentence_transformer.encode([caption])\n",
        "\n",
        "        # Recognize text\n",
        "        recognized_text = recognize_text(text_recognition_model, input_tensor)\n",
        "        recognized_text_embedding = sentence_transformer.encode([recognized_text])\n",
        "\n",
        "        # Compute similarity scores\n",
        "        caption_similarity = cosine_similarity(prompt_embedding, caption_embedding)[0][0]\n",
        "        text_similarity = cosine_similarity(prompt_embedding, recognized_text_embedding)[0][0]\n",
        "\n",
        "        # Store max similarity score for this image\n",
        "        max_similarity = max(caption_similarity, text_similarity)\n",
        "        relevance_scores.append((max_similarity, image_path))\n",
        "\n",
        "    # Sort by relevance score\n",
        "    relevance_scores.sort(key=lambda x: x[0], reverse=True)\n",
        "    return relevance_scores[0]  # Most relevant image\n",
        "\n",
        "# Run Retrieval\n",
        "def main():\n",
        "    # Dataset folder path\n",
        "    folder_path = \"/path/to/your/dataset\"\n",
        "    image_paths = load_images_from_folder(folder_path)\n",
        "\n",
        "    text_prompt=input(\"Enter text prompt: \")\n",
        "\n",
        "    # Find the most relevant image\n",
        "    most_relevant_score, most_relevant_image_path = find_most_relevant_image(\n",
        "        text_prompt, image_paths, caption_model, text_recognition_model\n",
        "    )\n",
        "\n",
        "    print(f\"Most Relevant Image: {most_relevant_image_path} (Score: {most_relevant_score:.4f})\")\n",
        "    # Display the most relevant image\n",
        "    image = Image.open(most_relevant_image_path)\n",
        "    image.show()\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}